Author: Porter Fredrickson
Class: Functional and Parallel Programming
Mahidol University Trimester 3

### Overview ###
This project contains many different files and code which showcase my learning process (and parallelism) throughout this project. All files contain comments at the top of the code to show the goal and function of the specific file. This is to assist in understanding the learning processes I went through and what I was trying to complete. Important Project note: This project makes use of both Scala and Python. All files are contained within the repository, but the python code is within PythonScraper file folder and the Scala code is located in the src/main/scala file folder. However, some of the more important files which really showcase some of my success in this project are Scala's parallelScaperWithData, booksToScrapeSequential, AllSolutionsSequential, AllSolutionsParallel, and pythons realtyAutoScraper.py, phuketScraper.py, and proxy.py. 

### Evaluation and Success ###
Overall I would say that I was successful and met the requirements of this project. I had three measures of success in my project proposal which I will quickly summarize and explain how I met here. First, a portion of my success was dependent upon connecting a scraper to a website (or list of websites) and obtaining target data. Not only did I do this, but I did it multiple times and in multiple ways (with different technologies and programming languages). Next, a measure of success was if I could scrape multiple pages at the same time in parallel, or if I could scrape the same data from a page in parallel. I had great success (especially in scala) with scraping multiple pages at the same time in parallel, and I also did do a little bit of parallel scraping from one page (Read more in results below). I also said success could come in the form of getting blocked faster from a web page, which this did in fact occur when trying to reference 1000 book prices on Amazon, so in a way that was a success too. Finally, my most important goals for the project were to learn about web scraping and to produce some form of concurrent speedup. I was successful in this endeavor because I had absolutely no idea what web scraping was and what it entailed until I proposed this project and now I have learned quite a lot. I also showcased many different forms of parallel speedups and solutions that would actually be useful while web scraping so overall this project was a success and I would evaluate myself highly. My high evaluation is not based off only my successes in achieving parallel speedup, but also my failures, and the sheer amount I learned during this project as well as my great amount of time dedicated to completing the project. I think it's worth recognizing that before I could even attempt concurrency, I had to learn how to web scrape, and do quite a bit browsing to find target sites that were optimal for scraping and showcasing parallel speedup. Finally, I worked on this project every single day since it was assigned (except for like two days when I had exams but I made up the time later) which shows great dedication and adherence to the time allocation we were supposed to be following for this project. My GitHub commits will show the daily work I put in, and from what I've heard, it doesn't sound like all of my peers were as disciplined following daily contributions to the project so I'm proud of my consistency. 

Please checkout my presentation file which also is in the GitHub. There's great stats and showcase of what I have completed in the project. Also, it may be a helpful reminder of what I presented and said in the final class.

-------- Results break down and details below ----------

#### Books to scrape ####
In this part of my project I used Scala and Jsoup to scrape books.toscrap.com which is a website that promotes web scraping (allowed me to not worry about getting blocked). For this part of the project I placed all of the links (1000 in total) for every book page on books.toscrape in a csv file. Then I used Scala and Jsoup to scrape data from each of the links in the csv file. The sequential version only scrapes the title and price of each of the 1000 books. I recorded this process to take 315 seconds for the sequential scraper. The parallel scraper uses Scalas map function and creates futures with the Jsoup connections. The parallel version scrapes even more data from the page than sequential (like stock) and makes aggregate calculations with the data as well. All said and done, when the parallel version was tested after the sequential version, it scraped the same 1000 links in very fast 36 seconds. That's a whopping 8.75x speedup which was quite impressive. 


##### Proxies #####
Proxy.py file is the file which concurrently checks a large list of free proxies. These proxies are obtained from a free proxy list API. Verifying the validity of the free proxies is important, because most of them don't work. The sequential version of this task can be seen in proxy_sequential.py. The list of proxies received from the API call is Usually about 1200 + proxies to check. The parallel version can check all proxies in around 193 seconds. That may not sound blazingly fast, but it is massive speedup compared to the sequential version. The reason that it takes 193 seconds for the parallel version is because I have to wait for some amount of time (I found 3 seconds to be good) To see if the proxy can actually connect to a target website. For the parallel version I make use of python futures, the thread pool executor, and the map function to achieve great speedup compared to the sequential version of the proxy checker which is unbearably slow. The sequential version takes so long, that my computer has always timed out before the program has completed. However, I still can showcase the parallel versions massive success, because in 193 seconds the parallel version checked over 1200 proxies and determined their validity. However, in the same amount of time, the sequential version had only checked the validity of 134 proxies. Valid proxies for both solutions are written to the working_proxies.txt file. Either delete the contents of the file, or scroll to the bottom to see working proxies being written to the file. In the parallel solution I put a thread lock on writes so that no important working proxies will be lost to race conditions. (The longest I waited for sequential Proxy is 10 min 37 seconds and it only checked 367 proxies so I think the sequential version would take somewhere around 45 minutes in total. That means my parallel version achieves better than 10x speedup!! Quite cool to me.)

### Selenium, Realty scraper, and all solutions. ###
For this part of the project I had a tiny bit more time and I wanted to take my crack at scraping and loading dynamically loaded web content. I took a shot at dynamically loading content with Scala by trying to integrate Selenium web driver as well as other browser automation technologies. However, I found that there was little documentation and instructions on how to integrate the latest versions of selenium with the latest versions of scala. As such, I turned to Python which is setup much better for integrating with Selenium. After taking a while to actually download and find out how to use selenium for web scraping (I read documentation and watched some tutorial videos), I turned my sites to scraping data from a Thai Realty company called www.fivestars-thailand.com. With selenium web driver I was able to dynamically load more listings by finding show more button after inspecting the web elements. After locating the button and through some trial and error I was able to automate clicking this button. Then I introduce a timeout to let the new content load. Once I've loaded all the pages, then I can scrape all of the loaded listing details from the page. Loading the dynamic content is quite sequential in nature, but I actually realized that elements could be scraped in scraped in parallel so I implemented versions web driver that do the scraping in parallels well. Ultimately, I decided another way to add parallelism and show speedup would be to combine a number of my scraping projects all at once to demonstrate concurrency. I also wanted to bring things back to Scala as much as possible. As such I created AllSolutions Sequential and AllSolutionsParallel. These two files run the 1000 books to scrape code and call the python scripts from Scala to scrape realty in Bangkok and Phuket. For the parallel version, all of these tasks were wrapped in futures and executed concurrently (you can see the two web drivers load content at the same time which is pretty cool). Once again, I achieve massive speedup with concurrency and futures in Scala. It took the sequential version 370 seconds to complete all three tasks while the parallel version ran in just 54 seconds. This is an Impressive 6.85X speedup and once again shows how useful concurrency is in web scraping. 



Note that occasionally Selenium Web Driver will just fail to run or during execution. Though it is rare, 
in this case rerun the code. 


Running the Code Details: 
For the Python code, I've made a python virtual environment. This should make it easier to run the code because requirements for library downloads and packages are specified in the requirements.txt. 
I also tried to code everything in python such that the least amount of packages will be needed for it to be run on another Computer. After cloning the repo, I think all you should need to do is activate the python
Virtual environment. This can be done in the terminal by navigating to the project directory and typing 
"$ .\venv\Scripts\activate" to activate in windows and "$ source venv/bin/activate" if you are on Mac. Afterwards, install the requirements for the project with pip and the following command: 
"$pip install -r requirements.txt"



